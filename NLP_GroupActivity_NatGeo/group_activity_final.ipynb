{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.expanduser('~/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "para1 = nltk.data.load('para1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anniversary editions have the feel of a graduation: a year of studious slogging (of which, truth be told, my team and I do very little) and madcap fun (which we only wish we could indulge in more) rounded off with a sense of achievement and lingering anxiety. There’s pride that National Geographic Traveller India has lived to see another day, and in today’s precarious media landscape, that should account for something. Then the gnawing question: did we get it right?\\r\\nWhen it comes to travel, is there a right or a wrong way to do it? Early this month, The New York Times unearthed Albert Einstein’s entries of his journeys around Asia and discovered a surprising side to the Nobel Prize winner. About his time in mainland China he wrote, “In the air there is a stench of never-ending manifold variety.” The people, he found, were “industrious, filthy, obtuse…” Travel often functions as a Rorschach test of biases.\\r\\nSome are acutely aware of this and spend their time making amends. Anthony Bourdain’s recent passing prompted glowing tributes from around the world to his open-minded exploration of parts and cultures unknown. There are others who stand their ground: If a traveller’s true sentiments veer towards exotification, maybe it should stay so. Read Indian-American author Akhil Sharma’s recounting of a fortnight in Japan, featured in this issue, for a perfect example. The counter to which, also in this edition, are the observations of three insiders on their hometowns: Member of Parliament Shashi Tharoor sings paeans to Thiruvananthapuram, musician Raghu Dixit toasts Mysore and writer Janice Pariat reminisces about Shillong. The “how” of travel is a matter of debate too. Dyed-in-the-wool snobs harp on about authenticity and immersing yourself in local culture. The more you are inconvenienced, the more real your journey. To which, casual travellers will respond with, “I will take my comfortable stay in a nice hotel, thank you very much.” NGTI’s sixth anniversary is a distillation of these myriad attitudes to travel. In their own way, our writers show you the “right way to do it.”\\\\ Our centrepiece is the “Smart Hacks” section that features an expert’s take on how best to navigate a place. Lensman Abhishek Hajela, a regular visitor to Ladakh, gives readers a glimpse into getting drool-worthy shots in Ladakh. Vaishali Dinakaran, an avowed gearhead, has the lowdown on grappling with Europe by road. Kaushal Karkhanis decodes solo backpacking in South America for the faraway dreamers. Chinmai Gupta offers a guide through that most “mystical” of institutions—a London nightclub. And if these stories are only a reminder of how ill-prepared your wallet is to go anywhere, we have solutions for that, too. As to whether we got it right, we have another year to fuss over that.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "para2 = nltk.data.load('para2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our year-end edition toasts ultra-indulgence while travelling, featuring itineraries that many will know to be out of their financial reach. In producing these narratives, I was struck by a contrast. Travel today is dominated by minimalists or downsizers, those who preach the gospel of “hard-knock wanderlust.” And they almost always reap universal admiration. They are characters to aspire to, examples of made-for-Instagram sayings such as, “All you need is a backpack” or “#MotorcycleDiaries.” Unable to join these gallivanting philosophers, others marvel at their brave rebellion—oh, to give up the predictability of overpriced tourist traps someday, they sigh. In this context, luxury travel evokes a Molotov cocktail of feelings. A billionaire on a sailboat hosting Jazz Age-style revelries in the French Riviera is inevitably setting himself up for mockery. The heiress, who flits off to shopping holidays in Milan and Dubai, might as well buy an extra pair of sunglasses for the shade directed her way. Extravagance passes muster if it panders to affordability. In the last few years, it has become intertwined with entitlement, a radioactive pejorative today. Upper-class travel doesn’t deserve this slight. As more astute aesthetes have reminded us in the past, refined tastes don’t have to be gauche. Living like royalty might have its privileges but it also spurs a temperament for beauty, grace and sensuality, which is why travellers will always fork out top penny for a night in Rajasthan’s many palace stays. Wealth facilitates the kind of understated exclusivity seen in the English countryside’s several castles or manors, once a venue for elegant ballroom dances. Luxury could also simply mean time well spent—or doing nothing—floating atop a sundeck in an unending stretch of the ocean. Professional travel writers are lucky to be granted access to these private paradises and, in December’s magazine, a handful of them have returned with colourful dispatches. One writer enjoys a happy recreational bubble in the Maldives, another is privy to up close views of big game in Botswana. There is also a roundup of New York’s elite food and drinking haunts, and coverage of the maiden cruise between Mumbai and Goa. All these retreats promise a hedonistic binge: grand feasts of fine wine and champagne, and views hidden from the typical trails. Some of them will test your purse-strings but think of Holly Golightly. She couldn’t lay claim to real Tiffany’s jewels but that never stopped her from getting her heart’s fill, standing outside the window.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "para3 = nltk.data.load('para3.txt')\n",
    "para4 = nltk.data.load('para4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens_para1 = word_tokenize(para1)\n",
    "tokens_para2 = word_tokenize(para2)\n",
    "tokens_para3 = word_tokenize(para3)\n",
    "tokens_para4 = word_tokenize(para4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_para1_wsw = [word for word in tokens_para1 if word.isalpha()]\n",
    "words_para2_wsw = [word for word in tokens_para2 if word.isalpha()]\n",
    "words_para3_wsw = [word for word in tokens_para3 if word.isalpha()]\n",
    "words_para4_wsw = [word for word in tokens_para4 if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anniversary',\n",
       " 'editions',\n",
       " 'have',\n",
       " 'the',\n",
       " 'feel',\n",
       " 'of',\n",
       " 'a',\n",
       " 'graduation',\n",
       " 'a',\n",
       " 'year',\n",
       " 'of',\n",
       " 'studious',\n",
       " 'slogging',\n",
       " 'of',\n",
       " 'which',\n",
       " 'truth',\n",
       " 'be',\n",
       " 'told',\n",
       " 'my',\n",
       " 'team',\n",
       " 'and',\n",
       " 'I',\n",
       " 'do',\n",
       " 'very',\n",
       " 'little',\n",
       " 'and',\n",
       " 'madcap',\n",
       " 'fun',\n",
       " 'which',\n",
       " 'we',\n",
       " 'only',\n",
       " 'wish',\n",
       " 'we',\n",
       " 'could',\n",
       " 'indulge',\n",
       " 'in',\n",
       " 'more',\n",
       " 'rounded',\n",
       " 'off',\n",
       " 'with',\n",
       " 'a',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'achievement',\n",
       " 'and',\n",
       " 'lingering',\n",
       " 'anxiety',\n",
       " 'There',\n",
       " 's',\n",
       " 'pride',\n",
       " 'that',\n",
       " 'National',\n",
       " 'Geographic',\n",
       " 'Traveller',\n",
       " 'India',\n",
       " 'has',\n",
       " 'lived',\n",
       " 'to',\n",
       " 'see',\n",
       " 'another',\n",
       " 'day',\n",
       " 'and',\n",
       " 'in',\n",
       " 'today',\n",
       " 's',\n",
       " 'precarious',\n",
       " 'media',\n",
       " 'landscape',\n",
       " 'that',\n",
       " 'should',\n",
       " 'account',\n",
       " 'for',\n",
       " 'something',\n",
       " 'Then',\n",
       " 'the',\n",
       " 'gnawing',\n",
       " 'question',\n",
       " 'did',\n",
       " 'we',\n",
       " 'get',\n",
       " 'it',\n",
       " 'right',\n",
       " 'When',\n",
       " 'it',\n",
       " 'comes',\n",
       " 'to',\n",
       " 'travel',\n",
       " 'is',\n",
       " 'there',\n",
       " 'a',\n",
       " 'right',\n",
       " 'or',\n",
       " 'a',\n",
       " 'wrong',\n",
       " 'way',\n",
       " 'to',\n",
       " 'do',\n",
       " 'it',\n",
       " 'Early',\n",
       " 'this',\n",
       " 'month',\n",
       " 'The',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Times',\n",
       " 'unearthed',\n",
       " 'Albert',\n",
       " 'Einstein',\n",
       " 's',\n",
       " 'entries',\n",
       " 'of',\n",
       " 'his',\n",
       " 'journeys',\n",
       " 'around',\n",
       " 'Asia',\n",
       " 'and',\n",
       " 'discovered',\n",
       " 'a',\n",
       " 'surprising',\n",
       " 'side',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Nobel',\n",
       " 'Prize',\n",
       " 'winner',\n",
       " 'About',\n",
       " 'his',\n",
       " 'time',\n",
       " 'in',\n",
       " 'mainland',\n",
       " 'China',\n",
       " 'he',\n",
       " 'wrote',\n",
       " 'In',\n",
       " 'the',\n",
       " 'air',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'stench',\n",
       " 'of',\n",
       " 'manifold',\n",
       " 'The',\n",
       " 'people',\n",
       " 'he',\n",
       " 'found',\n",
       " 'were',\n",
       " 'industrious',\n",
       " 'filthy',\n",
       " 'Travel',\n",
       " 'often',\n",
       " 'functions',\n",
       " 'as',\n",
       " 'a',\n",
       " 'Rorschach',\n",
       " 'test',\n",
       " 'of',\n",
       " 'biases',\n",
       " 'Some',\n",
       " 'are',\n",
       " 'acutely',\n",
       " 'aware',\n",
       " 'of',\n",
       " 'this',\n",
       " 'and',\n",
       " 'spend',\n",
       " 'their',\n",
       " 'time',\n",
       " 'making',\n",
       " 'amends',\n",
       " 'Anthony',\n",
       " 'Bourdain',\n",
       " 's',\n",
       " 'recent',\n",
       " 'passing',\n",
       " 'prompted',\n",
       " 'glowing',\n",
       " 'tributes',\n",
       " 'from',\n",
       " 'around',\n",
       " 'the',\n",
       " 'world',\n",
       " 'to',\n",
       " 'his',\n",
       " 'exploration',\n",
       " 'of',\n",
       " 'parts',\n",
       " 'and',\n",
       " 'cultures',\n",
       " 'unknown',\n",
       " 'There',\n",
       " 'are',\n",
       " 'others',\n",
       " 'who',\n",
       " 'stand',\n",
       " 'their',\n",
       " 'ground',\n",
       " 'If',\n",
       " 'a',\n",
       " 'traveller',\n",
       " 's',\n",
       " 'true',\n",
       " 'sentiments',\n",
       " 'veer',\n",
       " 'towards',\n",
       " 'exotification',\n",
       " 'maybe',\n",
       " 'it',\n",
       " 'should',\n",
       " 'stay',\n",
       " 'so',\n",
       " 'Read',\n",
       " 'author',\n",
       " 'Akhil',\n",
       " 'Sharma',\n",
       " 's',\n",
       " 'recounting',\n",
       " 'of',\n",
       " 'a',\n",
       " 'fortnight',\n",
       " 'in',\n",
       " 'Japan',\n",
       " 'featured',\n",
       " 'in',\n",
       " 'this',\n",
       " 'issue',\n",
       " 'for',\n",
       " 'a',\n",
       " 'perfect',\n",
       " 'example',\n",
       " 'The',\n",
       " 'counter',\n",
       " 'to',\n",
       " 'which',\n",
       " 'also',\n",
       " 'in',\n",
       " 'this',\n",
       " 'edition',\n",
       " 'are',\n",
       " 'the',\n",
       " 'observations',\n",
       " 'of',\n",
       " 'three',\n",
       " 'insiders',\n",
       " 'on',\n",
       " 'their',\n",
       " 'hometowns',\n",
       " 'Member',\n",
       " 'of',\n",
       " 'Parliament',\n",
       " 'Shashi',\n",
       " 'Tharoor',\n",
       " 'sings',\n",
       " 'paeans',\n",
       " 'to',\n",
       " 'Thiruvananthapuram',\n",
       " 'musician',\n",
       " 'Raghu',\n",
       " 'Dixit',\n",
       " 'toasts',\n",
       " 'Mysore',\n",
       " 'and',\n",
       " 'writer',\n",
       " 'Janice',\n",
       " 'Pariat',\n",
       " 'reminisces',\n",
       " 'about',\n",
       " 'Shillong',\n",
       " 'The',\n",
       " 'how',\n",
       " 'of',\n",
       " 'travel',\n",
       " 'is',\n",
       " 'a',\n",
       " 'matter',\n",
       " 'of',\n",
       " 'debate',\n",
       " 'too',\n",
       " 'snobs',\n",
       " 'harp',\n",
       " 'on',\n",
       " 'about',\n",
       " 'authenticity',\n",
       " 'and',\n",
       " 'immersing',\n",
       " 'yourself',\n",
       " 'in',\n",
       " 'local',\n",
       " 'culture',\n",
       " 'The',\n",
       " 'more',\n",
       " 'you',\n",
       " 'are',\n",
       " 'inconvenienced',\n",
       " 'the',\n",
       " 'more',\n",
       " 'real',\n",
       " 'your',\n",
       " 'journey',\n",
       " 'To',\n",
       " 'which',\n",
       " 'casual',\n",
       " 'travellers',\n",
       " 'will',\n",
       " 'respond',\n",
       " 'with',\n",
       " 'I',\n",
       " 'will',\n",
       " 'take',\n",
       " 'my',\n",
       " 'comfortable',\n",
       " 'stay',\n",
       " 'in',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'hotel',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'very',\n",
       " 'NGTI',\n",
       " 's',\n",
       " 'sixth',\n",
       " 'anniversary',\n",
       " 'is',\n",
       " 'a',\n",
       " 'distillation',\n",
       " 'of',\n",
       " 'these',\n",
       " 'myriad',\n",
       " 'attitudes',\n",
       " 'to',\n",
       " 'travel',\n",
       " 'In',\n",
       " 'their',\n",
       " 'own',\n",
       " 'way',\n",
       " 'our',\n",
       " 'writers',\n",
       " 'show',\n",
       " 'you',\n",
       " 'the',\n",
       " 'right',\n",
       " 'way',\n",
       " 'to',\n",
       " 'do',\n",
       " 'Our',\n",
       " 'centrepiece',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Smart',\n",
       " 'Hacks',\n",
       " 'section',\n",
       " 'that',\n",
       " 'features',\n",
       " 'an',\n",
       " 'expert',\n",
       " 's',\n",
       " 'take',\n",
       " 'on',\n",
       " 'how',\n",
       " 'best',\n",
       " 'to',\n",
       " 'navigate',\n",
       " 'a',\n",
       " 'place',\n",
       " 'Lensman',\n",
       " 'Abhishek',\n",
       " 'Hajela',\n",
       " 'a',\n",
       " 'regular',\n",
       " 'visitor',\n",
       " 'to',\n",
       " 'Ladakh',\n",
       " 'gives',\n",
       " 'readers',\n",
       " 'a',\n",
       " 'glimpse',\n",
       " 'into',\n",
       " 'getting',\n",
       " 'shots',\n",
       " 'in',\n",
       " 'Ladakh',\n",
       " 'Vaishali',\n",
       " 'Dinakaran',\n",
       " 'an',\n",
       " 'avowed',\n",
       " 'gearhead',\n",
       " 'has',\n",
       " 'the',\n",
       " 'lowdown',\n",
       " 'on',\n",
       " 'grappling',\n",
       " 'with',\n",
       " 'Europe',\n",
       " 'by',\n",
       " 'road',\n",
       " 'Kaushal',\n",
       " 'Karkhanis',\n",
       " 'decodes',\n",
       " 'solo',\n",
       " 'backpacking',\n",
       " 'in',\n",
       " 'South',\n",
       " 'America',\n",
       " 'for',\n",
       " 'the',\n",
       " 'faraway',\n",
       " 'dreamers',\n",
       " 'Chinmai',\n",
       " 'Gupta',\n",
       " 'offers',\n",
       " 'a',\n",
       " 'guide',\n",
       " 'through',\n",
       " 'that',\n",
       " 'most',\n",
       " 'mystical',\n",
       " 'of',\n",
       " 'London',\n",
       " 'nightclub',\n",
       " 'And',\n",
       " 'if',\n",
       " 'these',\n",
       " 'stories',\n",
       " 'are',\n",
       " 'only',\n",
       " 'a',\n",
       " 'reminder',\n",
       " 'of',\n",
       " 'how',\n",
       " 'your',\n",
       " 'wallet',\n",
       " 'is',\n",
       " 'to',\n",
       " 'go',\n",
       " 'anywhere',\n",
       " 'we',\n",
       " 'have',\n",
       " 'solutions',\n",
       " 'for',\n",
       " 'that',\n",
       " 'too',\n",
       " 'As',\n",
       " 'to',\n",
       " 'whether',\n",
       " 'we',\n",
       " 'got',\n",
       " 'it',\n",
       " 'right',\n",
       " 'we',\n",
       " 'have',\n",
       " 'another',\n",
       " 'year',\n",
       " 'to',\n",
       " 'fuss',\n",
       " 'over',\n",
       " 'that']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_para1_wsw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitra/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_para1 = [w.lower() for w in tokens_para1]\n",
    "tokens_para2 = [w.lower() for w in tokens_para2]\n",
    "tokens_para3 = [w.lower() for w in tokens_para3]\n",
    "tokens_para4 = [w.lower() for w in tokens_para4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_para1 = [w for w in words_para1_wsw if not w in stop_words]\n",
    "words_para2 = [w for w in words_para2_wsw if not w in stop_words]\n",
    "words_para3 = [w for w in words_para3_wsw if not w in stop_words]\n",
    "words_para4 = [w for w in words_para4_wsw if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anniversary',\n",
       " 'editions',\n",
       " 'feel',\n",
       " 'graduation',\n",
       " 'year',\n",
       " 'studious',\n",
       " 'slogging',\n",
       " 'truth',\n",
       " 'told',\n",
       " 'team',\n",
       " 'I',\n",
       " 'little',\n",
       " 'madcap',\n",
       " 'fun',\n",
       " 'wish',\n",
       " 'could',\n",
       " 'indulge',\n",
       " 'rounded',\n",
       " 'sense',\n",
       " 'achievement',\n",
       " 'lingering',\n",
       " 'anxiety',\n",
       " 'There',\n",
       " 'pride',\n",
       " 'National',\n",
       " 'Geographic',\n",
       " 'Traveller',\n",
       " 'India',\n",
       " 'lived',\n",
       " 'see',\n",
       " 'another',\n",
       " 'day',\n",
       " 'today',\n",
       " 'precarious',\n",
       " 'media',\n",
       " 'landscape',\n",
       " 'account',\n",
       " 'something',\n",
       " 'Then',\n",
       " 'gnawing',\n",
       " 'question',\n",
       " 'get',\n",
       " 'right',\n",
       " 'When',\n",
       " 'comes',\n",
       " 'travel',\n",
       " 'right',\n",
       " 'wrong',\n",
       " 'way',\n",
       " 'Early',\n",
       " 'month',\n",
       " 'The',\n",
       " 'New',\n",
       " 'York',\n",
       " 'Times',\n",
       " 'unearthed',\n",
       " 'Albert',\n",
       " 'Einstein',\n",
       " 'entries',\n",
       " 'journeys',\n",
       " 'around',\n",
       " 'Asia',\n",
       " 'discovered',\n",
       " 'surprising',\n",
       " 'side',\n",
       " 'Nobel',\n",
       " 'Prize',\n",
       " 'winner',\n",
       " 'About',\n",
       " 'time',\n",
       " 'mainland',\n",
       " 'China',\n",
       " 'wrote',\n",
       " 'In',\n",
       " 'air',\n",
       " 'stench',\n",
       " 'manifold',\n",
       " 'The',\n",
       " 'people',\n",
       " 'found',\n",
       " 'industrious',\n",
       " 'filthy',\n",
       " 'Travel',\n",
       " 'often',\n",
       " 'functions',\n",
       " 'Rorschach',\n",
       " 'test',\n",
       " 'biases',\n",
       " 'Some',\n",
       " 'acutely',\n",
       " 'aware',\n",
       " 'spend',\n",
       " 'time',\n",
       " 'making',\n",
       " 'amends',\n",
       " 'Anthony',\n",
       " 'Bourdain',\n",
       " 'recent',\n",
       " 'passing',\n",
       " 'prompted',\n",
       " 'glowing',\n",
       " 'tributes',\n",
       " 'around',\n",
       " 'world',\n",
       " 'exploration',\n",
       " 'parts',\n",
       " 'cultures',\n",
       " 'unknown',\n",
       " 'There',\n",
       " 'others',\n",
       " 'stand',\n",
       " 'ground',\n",
       " 'If',\n",
       " 'traveller',\n",
       " 'true',\n",
       " 'sentiments',\n",
       " 'veer',\n",
       " 'towards',\n",
       " 'exotification',\n",
       " 'maybe',\n",
       " 'stay',\n",
       " 'Read',\n",
       " 'author',\n",
       " 'Akhil',\n",
       " 'Sharma',\n",
       " 'recounting',\n",
       " 'fortnight',\n",
       " 'Japan',\n",
       " 'featured',\n",
       " 'issue',\n",
       " 'perfect',\n",
       " 'example',\n",
       " 'The',\n",
       " 'counter',\n",
       " 'also',\n",
       " 'edition',\n",
       " 'observations',\n",
       " 'three',\n",
       " 'insiders',\n",
       " 'hometowns',\n",
       " 'Member',\n",
       " 'Parliament',\n",
       " 'Shashi',\n",
       " 'Tharoor',\n",
       " 'sings',\n",
       " 'paeans',\n",
       " 'Thiruvananthapuram',\n",
       " 'musician',\n",
       " 'Raghu',\n",
       " 'Dixit',\n",
       " 'toasts',\n",
       " 'Mysore',\n",
       " 'writer',\n",
       " 'Janice',\n",
       " 'Pariat',\n",
       " 'reminisces',\n",
       " 'Shillong',\n",
       " 'The',\n",
       " 'travel',\n",
       " 'matter',\n",
       " 'debate',\n",
       " 'snobs',\n",
       " 'harp',\n",
       " 'authenticity',\n",
       " 'immersing',\n",
       " 'local',\n",
       " 'culture',\n",
       " 'The',\n",
       " 'inconvenienced',\n",
       " 'real',\n",
       " 'journey',\n",
       " 'To',\n",
       " 'casual',\n",
       " 'travellers',\n",
       " 'respond',\n",
       " 'I',\n",
       " 'take',\n",
       " 'comfortable',\n",
       " 'stay',\n",
       " 'nice',\n",
       " 'hotel',\n",
       " 'thank',\n",
       " 'NGTI',\n",
       " 'sixth',\n",
       " 'anniversary',\n",
       " 'distillation',\n",
       " 'myriad',\n",
       " 'attitudes',\n",
       " 'travel',\n",
       " 'In',\n",
       " 'way',\n",
       " 'writers',\n",
       " 'show',\n",
       " 'right',\n",
       " 'way',\n",
       " 'Our',\n",
       " 'centrepiece',\n",
       " 'Smart',\n",
       " 'Hacks',\n",
       " 'section',\n",
       " 'features',\n",
       " 'expert',\n",
       " 'take',\n",
       " 'best',\n",
       " 'navigate',\n",
       " 'place',\n",
       " 'Lensman',\n",
       " 'Abhishek',\n",
       " 'Hajela',\n",
       " 'regular',\n",
       " 'visitor',\n",
       " 'Ladakh',\n",
       " 'gives',\n",
       " 'readers',\n",
       " 'glimpse',\n",
       " 'getting',\n",
       " 'shots',\n",
       " 'Ladakh',\n",
       " 'Vaishali',\n",
       " 'Dinakaran',\n",
       " 'avowed',\n",
       " 'gearhead',\n",
       " 'lowdown',\n",
       " 'grappling',\n",
       " 'Europe',\n",
       " 'road',\n",
       " 'Kaushal',\n",
       " 'Karkhanis',\n",
       " 'decodes',\n",
       " 'solo',\n",
       " 'backpacking',\n",
       " 'South',\n",
       " 'America',\n",
       " 'faraway',\n",
       " 'dreamers',\n",
       " 'Chinmai',\n",
       " 'Gupta',\n",
       " 'offers',\n",
       " 'guide',\n",
       " 'mystical',\n",
       " 'London',\n",
       " 'nightclub',\n",
       " 'And',\n",
       " 'stories',\n",
       " 'reminder',\n",
       " 'wallet',\n",
       " 'go',\n",
       " 'anywhere',\n",
       " 'solutions',\n",
       " 'As',\n",
       " 'whether',\n",
       " 'got',\n",
       " 'right',\n",
       " 'another',\n",
       " 'year',\n",
       " 'fuss']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_para1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mitra/nltk_data'\n    - 'D:\\\\Downloads\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\Downloads\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\Downloads\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mitra\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mD:\\Downloads\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mitra/nltk_data'\n    - 'D:\\\\Downloads\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\Downloads\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\Downloads\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mitra\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b99388ebba15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlemmatized_para1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_para1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlemmatized_para2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_para2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlemmatized_para3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_para3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlemmatized_para4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_para4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-b99388ebba15>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlemmatized_para1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_para1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlemmatized_para2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_para2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlemmatized_para3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_para3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlemmatized_para4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_para4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\Anaconda\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Downloads\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\mitra/nltk_data'\n    - 'D:\\\\Downloads\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\Downloads\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\Downloads\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\mitra\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lemmatized_para1 = [lemmatizer.lemmatize(word) for word in words_para1]\n",
    "lemmatized_para2 = [lemmatizer.lemmatize(word) for word in words_para2]\n",
    "lemmatized_para3 = [lemmatizer.lemmatize(word) for word in words_para3]\n",
    "lemmatized_para4 = [lemmatizer.lemmatize(word) for word in words_para4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yellowbrick'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f65c4e52c05c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0myellowbrick\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFreqDistVisualizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdocs\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpara1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfeatures\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'yellowbrick'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "vectorizer = CountVectorizer()\n",
    "docs       = vectorizer.fit_transform([para1])\n",
    "features   = vectorizer.get_feature_names()\n",
    "\n",
    "visualizer = FreqDistVisualizer(features=features, orient='v')\n",
    "visualizer.fit(docs)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatized_para1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-726785a30b4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfdist_para1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_para1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfdist_para2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_para2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfdist_para3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_para3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfdist_para4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_para4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatized_para1' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist_para1 = FreqDist(lemmatized_para1)\n",
    "fdist_para2 = FreqDist(lemmatized_para2)\n",
    "fdist_para3 = FreqDist(lemmatized_para3)\n",
    "fdist_para4 = FreqDist(lemmatized_para4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist_para1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c67898e7b1a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mttr_para1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdist_para1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_para1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mttr_para1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fdist_para1' is not defined"
     ]
    }
   ],
   "source": [
    "ttr_para1 = len(fdist_para1)*100/len(lemmatized_para1)\n",
    "ttr_para1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist_para2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-bbab3e890191>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mttr_para2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdist_para2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_para2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mttr_para2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fdist_para2' is not defined"
     ]
    }
   ],
   "source": [
    "ttr_para2 = len(fdist_para2)*100/len(lemmatized_para2)\n",
    "ttr_para2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist_para3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-b5621fe8351b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mttr_para3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdist_para3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_para3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mttr_para3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fdist_para3' is not defined"
     ]
    }
   ],
   "source": [
    "ttr_para3 = len(fdist_para3)*100/len(lemmatized_para3)\n",
    "ttr_para3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fdist_para4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-bab760dbacff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mttr_para4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdist_para4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatized_para4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mttr_para4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fdist_para4' is not defined"
     ]
    }
   ],
   "source": [
    "ttr_para4 = len(fdist_para4)*100/len(lemmatized_para4)\n",
    "ttr_para4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flesch formual for guaging readabilty of text : \n",
    "# Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW)\n",
    "# Here,\n",
    "# ASL = average sentence length (number of words divided by number of sentences)\n",
    "# ASW = average word length in syllables (number of syllables divided by number of words)\n",
    "\n",
    "# calculating for para1\n",
    "\n",
    "sentences = len(nltk.sent_tokenize(para1))\n",
    "words = len(word_tokenize(para1))\n",
    "\n",
    "ASL = words/sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def syllables(word):\n",
    "    #referred from stackoverflow.com/questions/14541303/count-the-number-of-syllables-in-a-word\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "syl_count = 0\n",
    "for word in words_para1_wsw:\n",
    "    syl_count+=syllables(word)\n",
    "\n",
    "ASW = syl_count/words\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syl_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3109090909090908"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ASW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.55709090909093"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 206.835 - (1.015 * ASL) - (84.6 * ASW)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flesch score is 70.55 which is equivalent to school grade level 8. \n",
    "#So the readablity is quite good as its very easy for an average adult to read and understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textstat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-0b2ead3cea18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## SMOG Score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtextstat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msmog_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SMOG Score for para 1: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmog_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpara1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textstat'"
     ]
    }
   ],
   "source": [
    "## SMOG Score\n",
    "\n",
    "from textstat import smog_index\n",
    "\n",
    "print(\"SMOG Score for para 1: \", smog_index(para1))\n",
    "print(\"SMOG Score for para 2: \", smog_index(para2))\n",
    "print(\"SMOG Score for para 3: \", smog_index(para3))\n",
    "print(\"SMOG Score for para 4: \", smog_index(para4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of Readability Index Using Gunning-Fog Index\n",
    "'''Contributed By: Abhirupa Mitra'''\n",
    "\n",
    "# Return total Difficult Words in a text \n",
    "def difficult_words(text):   \n",
    "    # Find all words in the text \n",
    "    words = [] \n",
    "    sentences = break_sentences(text) \n",
    "    for sentence in sentences: \n",
    "        words += [str(token) for token in sentence] \n",
    "  \n",
    "    # difficult words are those with syllables >= 2 \n",
    "    # easy_word_set is provide by Textstat as  \n",
    "    # a list of common words \n",
    "    diff_words_set = set() \n",
    "    ew = textstat.textstat._textstatistics__get_lang_easy_words() \n",
    "    for word in words: \n",
    "        syllable_count = textstatistics().syllable_count(word) \n",
    "        if word not in ew and syllable_count >= 2: \n",
    "            diff_words_set.add(word) \n",
    "  \n",
    "    return len(diff_words_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find word count for given text\n",
    "def word_count(text): \n",
    "    sentences = break_sentences(text) \n",
    "    words = 0\n",
    "    for sentence in sentences: \n",
    "        words += len([token for token in sentence]) \n",
    "    return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "def break_sentences(text): \n",
    "    nlp = spacy.load('en_core_web_sm') \n",
    "    doc = nlp(text) \n",
    "    return doc.sents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Master method for calculation of gunning fog index\n",
    "def gunning_fog(text): \n",
    "    per_diff_words = (difficult_words(text) / word_count(text) * 100) + 5\n",
    "    grade = 0.4 * (avg_sentence_length(text) + per_diff_words) \n",
    "    return grade \n",
    "\n",
    "#Find average sentence length of all text in the paragraph\n",
    "def avg_sentence_length(text): \n",
    "    words = word_count(text) \n",
    "    sentences = sentence_count(text) \n",
    "    average_sentence_length = float(words / sentences) \n",
    "    return average_sentence_length \n",
    "\n",
    "# Returns the number of sentences in the text \n",
    "def sentence_count(text): \n",
    "    sentences = break_sentences(text) \n",
    "    return len(str(sentences) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gunning fog for para1: 16.499557522123894\n",
      "Gunning fog for para1: 15.79795918367347\n",
      "Gunning fog for para1: 15.910885860306644\n",
      "Gunning fog for para1: 14.684596622889305\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "# from textstat.textstat import textstatistics, easy_word_set, legacy_round \n",
    "# textstat._textstatistics__get_lang_easy_words()\n",
    "print(\"Gunning fog for para1:\",gunning_fog(para1))\n",
    "print(\"Gunning fog for para1:\",gunning_fog(para2))\n",
    "print(\"Gunning fog for para1:\",gunning_fog(para3))\n",
    "print(\"Gunning fog for para1:\",gunning_fog(para4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cosine Similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "paras = [para1,para2,para3,para4]\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(paras)\n",
    "cosine_val = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.52955552, 0.51896727, 0.51177251]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angle between the various text\n",
    "\n",
    "import math\n",
    "def angle_between(angle):\n",
    "    angle_in_radians = math.acos(angle)\n",
    "    return math.degrees(angle_in_radians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angle between doc 1 and doc 2:  58.0245722696079\n",
      "Angle between doc 1 and doc 3:  58.73699683105631\n",
      "Angle between doc 1 and doc 4:  59.2180318660501\n"
     ]
    }
   ],
   "source": [
    "print(\"Angle between doc 1 and doc 2: \", angle_between(cosine_val[0][1]))\n",
    "print(\"Angle between doc 1 and doc 3: \", angle_between(cosine_val[0][2]))\n",
    "print(\"Angle between doc 1 and doc 4: \", angle_between(cosine_val[0][3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
